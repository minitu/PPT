//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24330188
// Cuda compilation tools, release 9.2, V9.2.148
// Based on LLVM 3.4svn
//

.version 6.2
.target sm_70
.address_size 64

	// .weak	cudaMalloc
.weak .shared .align 8 .b8 _ZZN4RAJA4cuda4impl12block_reduceINS_6reduce3sumImEEmEET0_S6_S6_E2sd[256];

.weak .func  (.param .b32 func_retval0) cudaMalloc(
	.param .b64 cudaMalloc_param_0,
	.param .b64 cudaMalloc_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaFuncGetAttributes
.weak .func  (.param .b32 func_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaDeviceGetAttribute
.weak .func  (.param .b32 func_retval0) cudaDeviceGetAttribute(
	.param .b64 cudaDeviceGetAttribute_param_0,
	.param .b32 cudaDeviceGetAttribute_param_1,
	.param .b32 cudaDeviceGetAttribute_param_2
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaGetDevice
.weak .func  (.param .b32 func_retval0) cudaGetDevice(
	.param .b64 cudaGetDevice_param_0
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessor
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessor(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_3
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_3,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_4
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z7myisnand
.visible .func  (.param .b32 func_retval0) _Z7myisnand(
	.param .b64 _Z7myisnand_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [_Z7myisnand_param_0];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p1, %fd2, 0d7FF0000000000000;
	selp.u32	%r1, 1, 0, %p1;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2_
.weak .entry _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2_(
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_0[8],
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_1[8],
	.param .u64 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_0];
	ld.param.u64 	%rd3, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_1];
	ld.param.u64 	%rd4, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray11set_to_zeroEvEUlmE_lEEvT1_T0_T2__param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.s64	%p1, %rd1, %rd4;
	@%p1 bra 	BB7_2;

	cvta.to.global.u64 	%rd5, %rd2;
	add.s64 	%rd6, %rd1, %rd3;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd5, %rd7;
	mov.u64 	%rd9, 0;
	st.global.u64 	[%rd8], %rd9;

BB7_2:
	ret;
}

	// .weak	_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2_
.weak .entry _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2_(
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_0[16],
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_1[8],
	.param .u64 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.f64 	%fd1, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_0+8];
	ld.param.u64 	%rd2, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_0];
	ld.param.u64 	%rd3, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_1];
	ld.param.u64 	%rd4, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray9set_valueEdEUlmE_lEEvT1_T0_T2__param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.s64	%p1, %rd1, %rd4;
	@%p1 bra 	BB8_2;

	cvta.to.global.u64 	%rd5, %rd2;
	add.s64 	%rd6, %rd1, %rd3;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd5, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB8_2:
	ret;
}

	// .weak	_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2_
.weak .entry _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2_(
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0[64],
	.param .align 8 .b8 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_1[8],
	.param .u64 _ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<202>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<142>;


	ld.param.u64 	%rd33, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0+40];
	ld.param.u64 	%rd138, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0+32];
	ld.param.u64 	%rd30, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0+16];
	ld.param.u64 	%rd29, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0+8];
	ld.param.u64 	%rd28, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0];
	ld.param.u64 	%rd34, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_0+48];
	ld.param.u64 	%rd35, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_1];
	ld.param.u64 	%rd36, [_ZN4RAJA6policy4cuda4impl18forall_cuda_kernelILm256ENS_9Iterators16numeric_iteratorIllPlEEZN6Sarray10count_nansEvEUlmE_lEEvT1_T0_T2__param_2];
	cvta.to.global.u64 	%rd2, %rd34;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r20, %r1, %r2, %r3;
	cvt.u64.u32	%rd3, %r20;
	setp.ge.s64	%p1, %rd3, %rd36;
	mov.u64 	%rd130, %rd138;
	@%p1 bra 	BB9_3;

	cvta.to.global.u64 	%rd37, %rd28;
	add.s64 	%rd38, %rd3, %rd35;
	shl.b64 	%rd39, %rd38, 3;
	add.s64 	%rd40, %rd37, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	abs.f64 	%fd2, %fd1;
	setp.le.f64	%p2, %fd2, 0d7FF0000000000000;
	mov.u64 	%rd130, %rd138;
	@%p2 bra 	BB9_3;

	add.s64 	%rd130, %rd138, 1;

BB9_3:
	setp.ne.s64	%p3, %rd29, 0;
	@%p3 bra 	BB9_35;

	mov.u32 	%r21, %nctaid.y;
	mov.u32 	%r4, %nctaid.x;
	mul.lo.s32 	%r5, %r21, %r4;
	mov.u32 	%r22, %nctaid.z;
	mul.lo.s32 	%r6, %r5, %r22;
	mov.u32 	%r23, %ntid.y;
	mul.lo.s32 	%r24, %r23, %r1;
	mov.u32 	%r25, %ntid.z;
	mul.lo.s32 	%r7, %r24, %r25;
	mov.u32 	%r26, %tid.y;
	mad.lo.s32 	%r27, %r26, %r1, %r3;
	mov.u32 	%r28, %tid.z;
	mad.lo.s32 	%r8, %r28, %r24, %r27;
	cvt.s64.s32	%rd41, %r8;
	shr.s64 	%rd42, %rd41, 63;
	shr.u64 	%rd43, %rd42, 59;
	add.s64 	%rd44, %rd41, %rd43;
	shr.u64 	%rd45, %rd44, 5;
	shl.b64 	%rd46, %rd45, 5;
	sub.s64 	%rd6, %rd41, %rd46;
	cvt.u32.u64	%r9, %rd6;
	cvt.u32.u64	%r10, %rd45;
	and.b32  	%r11, %r7, 31;
	setp.eq.s32	%p4, %r11, 0;
	@%p4 bra 	BB9_6;

	xor.b32  	%r49, %r8, 1;
	// inline asm
	mov.b64 {%r29,%r30}, %rd130;
	// inline asm
	mov.u32 	%r50, 31;
	mov.u32 	%r51, -1;
	shfl.sync.idx.b32 	%r32|%p5, %r30, %r49, %r50, %r51;
	shfl.sync.idx.b32 	%r31|%p6, %r29, %r49, %r50, %r51;
	// inline asm
	mov.b64 %rd48, {%r31,%r32};
	// inline asm
	setp.lt.s32	%p7, %r49, %r7;
	selp.b64	%rd57, %rd48, 0, %p7;
	add.s64 	%rd49, %rd57, %rd130;
	// inline asm
	mov.b64 {%r33,%r34}, %rd49;
	// inline asm
	xor.b32  	%r52, %r8, 2;
	shfl.sync.idx.b32 	%r36|%p8, %r34, %r52, %r50, %r51;
	shfl.sync.idx.b32 	%r35|%p9, %r33, %r52, %r50, %r51;
	// inline asm
	mov.b64 %rd50, {%r35,%r36};
	// inline asm
	setp.lt.s32	%p10, %r52, %r7;
	selp.b64	%rd58, %rd50, 0, %p10;
	add.s64 	%rd51, %rd58, %rd49;
	// inline asm
	mov.b64 {%r37,%r38}, %rd51;
	// inline asm
	xor.b32  	%r53, %r8, 4;
	shfl.sync.idx.b32 	%r40|%p11, %r38, %r53, %r50, %r51;
	shfl.sync.idx.b32 	%r39|%p12, %r37, %r53, %r50, %r51;
	// inline asm
	mov.b64 %rd52, {%r39,%r40};
	// inline asm
	setp.lt.s32	%p13, %r53, %r7;
	selp.b64	%rd59, %rd52, 0, %p13;
	add.s64 	%rd53, %rd59, %rd51;
	// inline asm
	mov.b64 {%r41,%r42}, %rd53;
	// inline asm
	xor.b32  	%r54, %r8, 8;
	shfl.sync.idx.b32 	%r44|%p14, %r42, %r54, %r50, %r51;
	shfl.sync.idx.b32 	%r43|%p15, %r41, %r54, %r50, %r51;
	// inline asm
	mov.b64 %rd54, {%r43,%r44};
	// inline asm
	setp.lt.s32	%p16, %r54, %r7;
	selp.b64	%rd60, %rd54, 0, %p16;
	add.s64 	%rd55, %rd60, %rd53;
	// inline asm
	mov.b64 {%r45,%r46}, %rd55;
	// inline asm
	xor.b32  	%r55, %r8, 16;
	shfl.sync.idx.b32 	%r48|%p17, %r46, %r55, %r50, %r51;
	shfl.sync.idx.b32 	%r47|%p18, %r45, %r55, %r50, %r51;
	// inline asm
	mov.b64 %rd56, {%r47,%r48};
	// inline asm
	setp.lt.s32	%p19, %r55, %r7;
	selp.b64	%rd61, %rd56, 0, %p19;
	add.s64 	%rd131, %rd61, %rd55;
	bra.uni 	BB9_7;

BB9_6:
	// inline asm
	mov.b64 {%r56,%r57}, %rd130;
	// inline asm
	mov.u32 	%r76, 31;
	mov.u32 	%r77, 1;
	mov.u32 	%r78, -1;
	shfl.sync.bfly.b32 	%r59|%p20, %r57, %r77, %r76, %r78;
	shfl.sync.bfly.b32 	%r58|%p21, %r56, %r77, %r76, %r78;
	// inline asm
	mov.b64 %rd63, {%r58,%r59};
	// inline asm
	add.s64 	%rd64, %rd63, %rd130;
	// inline asm
	mov.b64 {%r60,%r61}, %rd64;
	// inline asm
	mov.u32 	%r79, 2;
	shfl.sync.bfly.b32 	%r63|%p22, %r61, %r79, %r76, %r78;
	shfl.sync.bfly.b32 	%r62|%p23, %r60, %r79, %r76, %r78;
	// inline asm
	mov.b64 %rd65, {%r62,%r63};
	// inline asm
	add.s64 	%rd66, %rd65, %rd64;
	// inline asm
	mov.b64 {%r64,%r65}, %rd66;
	// inline asm
	mov.u32 	%r80, 4;
	shfl.sync.bfly.b32 	%r67|%p24, %r65, %r80, %r76, %r78;
	shfl.sync.bfly.b32 	%r66|%p25, %r64, %r80, %r76, %r78;
	// inline asm
	mov.b64 %rd67, {%r66,%r67};
	// inline asm
	add.s64 	%rd68, %rd67, %rd66;
	// inline asm
	mov.b64 {%r68,%r69}, %rd68;
	// inline asm
	mov.u32 	%r81, 8;
	shfl.sync.bfly.b32 	%r71|%p26, %r69, %r81, %r76, %r78;
	shfl.sync.bfly.b32 	%r70|%p27, %r68, %r81, %r76, %r78;
	// inline asm
	mov.b64 %rd69, {%r70,%r71};
	// inline asm
	add.s64 	%rd70, %rd69, %rd68;
	// inline asm
	mov.b64 {%r72,%r73}, %rd70;
	// inline asm
	mov.u32 	%r82, 16;
	shfl.sync.bfly.b32 	%r75|%p28, %r73, %r82, %r76, %r78;
	shfl.sync.bfly.b32 	%r74|%p29, %r72, %r82, %r76, %r78;
	// inline asm
	mov.b64 %rd71, {%r74,%r75};
	// inline asm
	add.s64 	%rd131, %rd71, %rd70;

BB9_7:
	shl.b32 	%r83, %r10, 3;
	mov.u32 	%r84, _ZZN4RAJA4cuda4impl12block_reduceINS_6reduce3sumImEEmEET0_S6_S6_E2sd;
	add.s32 	%r12, %r84, %r83;
	shl.b32 	%r85, %r9, 3;
	add.s32 	%r13, %r84, %r85;
	mov.u32 	%r86, %ctaid.z;
	mov.u32 	%r87, %ctaid.y;
	mad.lo.s32 	%r88, %r87, %r4, %r2;
	mad.lo.s32 	%r14, %r86, %r5, %r88;
	setp.lt.s32	%p30, %r7, 33;
	@%p30 bra 	BB9_15;

	setp.ne.s32	%p31, %r9, 0;
	@%p31 bra 	BB9_10;

	st.shared.u64 	[%r12], %rd131;

BB9_10:
	bar.sync 	0;
	setp.ne.s32	%p32, %r10, 0;
	@%p32 bra 	BB9_14;

	shl.b64 	%rd72, %rd6, 32;
	shr.s64 	%rd73, %rd72, 27;
	cvt.s64.s32	%rd74, %r7;
	setp.ge.s64	%p33, %rd73, %rd74;
	mov.u64 	%rd132, %rd138;
	@%p33 bra 	BB9_13;

	ld.shared.u64 	%rd132, [%r13];

BB9_13:
	// inline asm
	mov.b64 {%r89,%r90}, %rd132;
	// inline asm
	mov.u32 	%r109, 31;
	mov.u32 	%r110, 1;
	mov.u32 	%r111, -1;
	shfl.sync.bfly.b32 	%r92|%p34, %r90, %r110, %r109, %r111;
	shfl.sync.bfly.b32 	%r91|%p35, %r89, %r110, %r109, %r111;
	// inline asm
	mov.b64 %rd76, {%r91,%r92};
	// inline asm
	add.s64 	%rd77, %rd76, %rd132;
	// inline asm
	mov.b64 {%r93,%r94}, %rd77;
	// inline asm
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r96|%p36, %r94, %r112, %r109, %r111;
	shfl.sync.bfly.b32 	%r95|%p37, %r93, %r112, %r109, %r111;
	// inline asm
	mov.b64 %rd78, {%r95,%r96};
	// inline asm
	add.s64 	%rd79, %rd78, %rd77;
	// inline asm
	mov.b64 {%r97,%r98}, %rd79;
	// inline asm
	mov.u32 	%r113, 4;
	shfl.sync.bfly.b32 	%r100|%p38, %r98, %r113, %r109, %r111;
	shfl.sync.bfly.b32 	%r99|%p39, %r97, %r113, %r109, %r111;
	// inline asm
	mov.b64 %rd80, {%r99,%r100};
	// inline asm
	add.s64 	%rd81, %rd80, %rd79;
	// inline asm
	mov.b64 {%r101,%r102}, %rd81;
	// inline asm
	mov.u32 	%r114, 8;
	shfl.sync.bfly.b32 	%r104|%p40, %r102, %r114, %r109, %r111;
	shfl.sync.bfly.b32 	%r103|%p41, %r101, %r114, %r109, %r111;
	// inline asm
	mov.b64 %rd82, {%r103,%r104};
	// inline asm
	add.s64 	%rd83, %rd82, %rd81;
	// inline asm
	mov.b64 {%r105,%r106}, %rd83;
	// inline asm
	mov.u32 	%r115, 16;
	shfl.sync.bfly.b32 	%r108|%p42, %r106, %r115, %r109, %r111;
	shfl.sync.bfly.b32 	%r107|%p43, %r105, %r115, %r109, %r111;
	// inline asm
	mov.b64 %rd84, {%r107,%r108};
	// inline asm
	add.s64 	%rd131, %rd84, %rd83;

BB9_14:
	bar.sync 	0;

BB9_15:
	mov.u32 	%r200, 0;
	setp.ne.s32	%p44, %r8, 0;
	@%p44 bra 	BB9_17;

	add.s32 	%r117, %r6, -1;
	mul.wide.s32 	%rd85, %r14, 8;
	add.s64 	%rd86, %rd2, %rd85;
	st.global.u64 	[%rd86], %rd131;
	membar.gl;
	cvta.to.global.u64 	%rd87, %rd33;
	atom.global.inc.u32 	%r118, [%rd87], %r117;
	setp.eq.s32	%p45, %r118, %r117;
	selp.u32	%r200, 1, 0, %p45;

BB9_17:
	{ 
	.reg .pred 	%p1; 
	.reg .pred 	%p2; 
	setp.ne.u32 	%p1, %r200, 0; 
	bar.red.or.pred 	%p2, 0, %p1; 
	selp.u32 	%r17, 1, 0, %p2; 
	}
	setp.eq.s32	%p46, %r17, 0;
	@%p46 bra 	BB9_33;

	setp.ge.s32	%p47, %r8, %r6;
	mov.u64 	%rd136, %rd138;
	@%p47 bra 	BB9_21;

	mov.u32 	%r201, %r8;
	mov.u64 	%rd136, %rd138;

BB9_20:
	mul.wide.s32 	%rd88, %r201, 8;
	add.s64 	%rd89, %rd2, %rd88;
	ld.global.u64 	%rd90, [%rd89];
	add.s64 	%rd136, %rd90, %rd136;
	add.s32 	%r201, %r201, %r7;
	setp.lt.s32	%p48, %r201, %r6;
	@%p48 bra 	BB9_20;

BB9_21:
	@%p4 bra 	BB9_23;

	xor.b32  	%r139, %r8, 1;
	// inline asm
	mov.b64 {%r119,%r120}, %rd136;
	// inline asm
	mov.u32 	%r140, 31;
	mov.u32 	%r141, -1;
	shfl.sync.idx.b32 	%r122|%p50, %r120, %r139, %r140, %r141;
	shfl.sync.idx.b32 	%r121|%p51, %r119, %r139, %r140, %r141;
	// inline asm
	mov.b64 %rd92, {%r121,%r122};
	// inline asm
	setp.lt.s32	%p52, %r139, %r7;
	selp.b64	%rd101, %rd92, 0, %p52;
	add.s64 	%rd93, %rd101, %rd136;
	// inline asm
	mov.b64 {%r123,%r124}, %rd93;
	// inline asm
	xor.b32  	%r142, %r8, 2;
	shfl.sync.idx.b32 	%r126|%p53, %r124, %r142, %r140, %r141;
	shfl.sync.idx.b32 	%r125|%p54, %r123, %r142, %r140, %r141;
	// inline asm
	mov.b64 %rd94, {%r125,%r126};
	// inline asm
	setp.lt.s32	%p55, %r142, %r7;
	selp.b64	%rd102, %rd94, 0, %p55;
	add.s64 	%rd95, %rd102, %rd93;
	// inline asm
	mov.b64 {%r127,%r128}, %rd95;
	// inline asm
	xor.b32  	%r143, %r8, 4;
	shfl.sync.idx.b32 	%r130|%p56, %r128, %r143, %r140, %r141;
	shfl.sync.idx.b32 	%r129|%p57, %r127, %r143, %r140, %r141;
	// inline asm
	mov.b64 %rd96, {%r129,%r130};
	// inline asm
	setp.lt.s32	%p58, %r143, %r7;
	selp.b64	%rd103, %rd96, 0, %p58;
	add.s64 	%rd97, %rd103, %rd95;
	// inline asm
	mov.b64 {%r131,%r132}, %rd97;
	// inline asm
	xor.b32  	%r144, %r8, 8;
	shfl.sync.idx.b32 	%r134|%p59, %r132, %r144, %r140, %r141;
	shfl.sync.idx.b32 	%r133|%p60, %r131, %r144, %r140, %r141;
	// inline asm
	mov.b64 %rd98, {%r133,%r134};
	// inline asm
	setp.lt.s32	%p61, %r144, %r7;
	selp.b64	%rd104, %rd98, 0, %p61;
	add.s64 	%rd99, %rd104, %rd97;
	// inline asm
	mov.b64 {%r135,%r136}, %rd99;
	// inline asm
	xor.b32  	%r145, %r8, 16;
	shfl.sync.idx.b32 	%r138|%p62, %r136, %r145, %r140, %r141;
	shfl.sync.idx.b32 	%r137|%p63, %r135, %r145, %r140, %r141;
	// inline asm
	mov.b64 %rd100, {%r137,%r138};
	// inline asm
	setp.lt.s32	%p64, %r145, %r7;
	selp.b64	%rd105, %rd100, 0, %p64;
	add.s64 	%rd137, %rd105, %rd99;
	bra.uni 	BB9_24;

BB9_23:
	// inline asm
	mov.b64 {%r146,%r147}, %rd136;
	// inline asm
	mov.u32 	%r166, 31;
	mov.u32 	%r167, 1;
	mov.u32 	%r168, -1;
	shfl.sync.bfly.b32 	%r149|%p65, %r147, %r167, %r166, %r168;
	shfl.sync.bfly.b32 	%r148|%p66, %r146, %r167, %r166, %r168;
	// inline asm
	mov.b64 %rd107, {%r148,%r149};
	// inline asm
	add.s64 	%rd108, %rd107, %rd136;
	// inline asm
	mov.b64 {%r150,%r151}, %rd108;
	// inline asm
	mov.u32 	%r169, 2;
	shfl.sync.bfly.b32 	%r153|%p67, %r151, %r169, %r166, %r168;
	shfl.sync.bfly.b32 	%r152|%p68, %r150, %r169, %r166, %r168;
	// inline asm
	mov.b64 %rd109, {%r152,%r153};
	// inline asm
	add.s64 	%rd110, %rd109, %rd108;
	// inline asm
	mov.b64 {%r154,%r155}, %rd110;
	// inline asm
	mov.u32 	%r170, 4;
	shfl.sync.bfly.b32 	%r157|%p69, %r155, %r170, %r166, %r168;
	shfl.sync.bfly.b32 	%r156|%p70, %r154, %r170, %r166, %r168;
	// inline asm
	mov.b64 %rd111, {%r156,%r157};
	// inline asm
	add.s64 	%rd112, %rd111, %rd110;
	// inline asm
	mov.b64 {%r158,%r159}, %rd112;
	// inline asm
	mov.u32 	%r171, 8;
	shfl.sync.bfly.b32 	%r161|%p71, %r159, %r171, %r166, %r168;
	shfl.sync.bfly.b32 	%r160|%p72, %r158, %r171, %r166, %r168;
	// inline asm
	mov.b64 %rd113, {%r160,%r161};
	// inline asm
	add.s64 	%rd114, %rd113, %rd112;
	// inline asm
	mov.b64 {%r162,%r163}, %rd114;
	// inline asm
	mov.u32 	%r172, 16;
	shfl.sync.bfly.b32 	%r165|%p73, %r163, %r172, %r166, %r168;
	shfl.sync.bfly.b32 	%r164|%p74, %r162, %r172, %r166, %r168;
	// inline asm
	mov.b64 %rd115, {%r164,%r165};
	// inline asm
	add.s64 	%rd137, %rd115, %rd114;

BB9_24:
	@%p30 bra 	BB9_32;

	setp.ne.s32	%p76, %r9, 0;
	@%p76 bra 	BB9_27;

	st.shared.u64 	[%r12], %rd137;

BB9_27:
	bar.sync 	0;
	setp.ne.s32	%p77, %r10, 0;
	@%p77 bra 	BB9_31;

	shl.b64 	%rd116, %rd6, 32;
	shr.s64 	%rd117, %rd116, 27;
	cvt.s64.s32	%rd118, %r7;
	setp.ge.s64	%p78, %rd117, %rd118;
	@%p78 bra 	BB9_30;

	ld.shared.u64 	%rd138, [%r13];

BB9_30:
	// inline asm
	mov.b64 {%r173,%r174}, %rd138;
	// inline asm
	mov.u32 	%r193, 31;
	mov.u32 	%r194, 1;
	mov.u32 	%r195, -1;
	shfl.sync.bfly.b32 	%r176|%p79, %r174, %r194, %r193, %r195;
	shfl.sync.bfly.b32 	%r175|%p80, %r173, %r194, %r193, %r195;
	// inline asm
	mov.b64 %rd120, {%r175,%r176};
	// inline asm
	add.s64 	%rd121, %rd120, %rd138;
	// inline asm
	mov.b64 {%r177,%r178}, %rd121;
	// inline asm
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r180|%p81, %r178, %r196, %r193, %r195;
	shfl.sync.bfly.b32 	%r179|%p82, %r177, %r196, %r193, %r195;
	// inline asm
	mov.b64 %rd122, {%r179,%r180};
	// inline asm
	add.s64 	%rd123, %rd122, %rd121;
	// inline asm
	mov.b64 {%r181,%r182}, %rd123;
	// inline asm
	mov.u32 	%r197, 4;
	shfl.sync.bfly.b32 	%r184|%p83, %r182, %r197, %r193, %r195;
	shfl.sync.bfly.b32 	%r183|%p84, %r181, %r197, %r193, %r195;
	// inline asm
	mov.b64 %rd124, {%r183,%r184};
	// inline asm
	add.s64 	%rd125, %rd124, %rd123;
	// inline asm
	mov.b64 {%r185,%r186}, %rd125;
	// inline asm
	mov.u32 	%r198, 8;
	shfl.sync.bfly.b32 	%r188|%p85, %r186, %r198, %r193, %r195;
	shfl.sync.bfly.b32 	%r187|%p86, %r185, %r198, %r193, %r195;
	// inline asm
	mov.b64 %rd126, {%r187,%r188};
	// inline asm
	add.s64 	%rd127, %rd126, %rd125;
	// inline asm
	mov.b64 {%r189,%r190}, %rd127;
	// inline asm
	mov.u32 	%r199, 16;
	shfl.sync.bfly.b32 	%r192|%p87, %r190, %r199, %r193, %r195;
	shfl.sync.bfly.b32 	%r191|%p88, %r189, %r199, %r193, %r195;
	// inline asm
	mov.b64 %rd128, {%r191,%r192};
	// inline asm
	add.s64 	%rd137, %rd128, %rd127;

BB9_31:
	bar.sync 	0;

BB9_32:
	setp.eq.s32	%p89, %r8, 0;
	selp.b64	%rd130, %rd137, %rd130, %p89;

BB9_33:
	or.pred  	%p92, %p44, %p46;
	@%p92 bra 	BB9_35;

	cvta.to.global.u64 	%rd129, %rd30;
	st.global.u64 	[%rd129], %rd130;

BB9_35:
	ret;
}

	// .weak	_ZN3cub11EmptyKernelIvEEvv
.weak .entry _ZN3cub11EmptyKernelIvEEvv(

)
{



	ret;
}


